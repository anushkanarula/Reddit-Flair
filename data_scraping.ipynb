{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries and setting up client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import praw\n",
    "from praw.models import MoreComments\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client[\"reddit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id = '_Ij9InJJ1MqC-Q', client_secret = '9XF46Dd9icSEmAghFivwt9J9uGU', user_agent = 'Reddit WebScraping', username = 'anushkanarula01', password = '')\n",
    "subreddit = reddit.subreddit('india')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Reddit and Saving the data to MongoDB database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of all the flairs. These will be the keys in classification:\n",
    "flairs = [\"AskIndia\", \"Non-Political\", \"[R]eddiquette\", \"Scheduled\", \"Photography\", \"Science/Technology\",\"Politics\", \"Business/Finance\", \"Policy/Economy\", \"Sports\", \"Food\", \"AMA\"]\n",
    "\n",
    "for flair in flairs:\n",
    "    #The posts' data is collected by searching by the flair name in the list. Top 100 posts are recorded and stored for analysis.\n",
    "    relevant_subreddits = subreddit.search(f\"flair_name:{flair}\",limit=100)\n",
    "    for submission in relevant_subreddits:\n",
    "        posts = {\n",
    "        \"title\":str(submission.title),\n",
    "        \"score\":str(submission.score),\n",
    "        \"id\":str(submission.id),\n",
    "        \"url\":str(submission.url),\n",
    "        \"comms_num\":str(submission.num_comments),\n",
    "        \"created\":str(submission.created),\n",
    "        \"body\":str(submission.selftext),\n",
    "        \"author\":str(submission.author),\n",
    "        \"flair\":str(flair),\t\n",
    "        }\n",
    "\n",
    "#  Only top ten comments and their authors are considered. \n",
    "        submission.comments.replace_more(limit=None)\n",
    "        comment = ''\n",
    "        authors = ''\n",
    "        count = 0\n",
    "        for top_level_comment in submission.comments:\n",
    "            comment = comment + ' ' + top_level_comment.body\n",
    "            authors = authors + ' ' + str(top_level_comment.author)\n",
    "            count+=1     \n",
    "            if(count > 10):\n",
    "                break\n",
    "\n",
    "        posts[\"comment\"] = str(comment)\n",
    "        posts[\"authors\"] = str(authors)\n",
    "        result = db.posts.insert_one(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_by_space = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "replace_symbol = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
    "    text = text.lower() # lowercase text\n",
    "    text = replace_by_space.sub(' ', text) # replacing certain symbols by space in text\n",
    "    text = replace_symbol.sub('', text) # deleting symbols from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # removing STOPWORDS from text\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre-Processing and Saving the final data into a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = db.posts\n",
    "posts = pd.DataFrame(list(collection.find()))\n",
    "\n",
    "del posts['_id']\n",
    "\n",
    "posts['title'] = posts['title'].apply(str())\n",
    "posts['body'] = posts['body'].apply(str())\n",
    "posts['comment'] = posts['comment'].apply(str())\n",
    "\n",
    "posts['title'] = posts['title'].apply(clean_text)\n",
    "posts['body'] = posts['body'].apply(clean_text)\n",
    "posts['comment'] = posts['comment'].apply(clean_text)\n",
    "\n",
    "combined_features = posts[\"title\"] + posts[\"comment\"] + posts[\"url\"] + posts[\"body\"]\n",
    "posts = posts.assign(combined_features = combined_features)\n",
    "\n",
    "#saving the csv file\n",
    "posts.to_csv('data.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
